I'll provide Python code examples for each of the topics listed in the image.


---

1. What is Statistics?

Statistics is the science of collecting, organizing, analyzing, and interpreting data. In Python, we can use libraries like numpy, pandas, and scipy.stats for statistical analysis.

import numpy as np
import pandas as pd
from scipy import stats


---

2. Methods in Statistics – Descriptive vs Inferential

Descriptive Statistics summarizes data (mean, median, mode), while Inferential Statistics makes predictions.

data = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]

# Descriptive Statistics
mean_value = np.mean(data)
median_value = np.median(data)
mode_value = stats.mode(data)

print("Mean:", mean_value)
print("Median:", median_value)
print("Mode:", mode_value.mode[0])

# Inferential Statistics (t-test example)
sample1 = np.random.normal(loc=50, scale=10, size=30)
sample2 = np.random.normal(loc=55, scale=10, size=30)
t_stat, p_value = stats.ttest_ind(sample1, sample2)
print("T-statistic:", t_stat, "P-value:", p_value)


---

3. Types of Data – Qualitative vs Quantitative

Qualitative (Categorical) data represents categories, while Quantitative (Numerical) data represents numbers.

df = pd.DataFrame({
    'Name': ['Alice', 'Bob', 'Charlie'],
    'Gender': ['Female', 'Male', 'Male'],  # Qualitative
    'Age': [25, 30, 35],  # Quantitative
    'Salary': [50000, 60000, 70000]  # Quantitative
})

print(df)


---

4. Measures of Central Tendency

Mean, Median, Mode measure the center of data.

data = [1, 2, 3, 4, 5, 5, 6, 7, 8, 9, 10]

mean = np.mean(data)
median = np.median(data)
mode = stats.mode(data)

print("Mean:", mean)
print("Median:", median)
print("Mode:", mode.mode[0])


---

5. Measures of Dispersion

Variance and Standard Deviation measure data spread.

std_dev = np.std(data)
variance = np.var(data)

print("Standard Deviation:", std_dev)
print("Variance:", variance)


---

6. Measures of Skewness and Kurtosis

Skewness measures asymmetry, Kurtosis measures tail heaviness.

skewness = stats.skew(data)
kurtosis = stats.kurtosis(data)

print("Skewness:", skewness)
print("Kurtosis:", kurtosis)


---

7. Measures of Relationship

Correlation and Covariance measure relationships.

df = pd.DataFrame({
    'X': [10, 20, 30, 40, 50],
    'Y': [15, 25, 35, 45, 55]
})

correlation = df.corr()
covariance = df.cov()

print("Correlation:\n", correlation)
print("Covariance:\n", covariance)


---

8. Feature Scaling – Standardization vs Normalization

Standardization (Z-score) and Normalization (Min-Max Scaling).

from sklearn.preprocessing import StandardScaler, MinMaxScaler

scaler = StandardScaler()
normalized_scaler = MinMaxScaler()

data_scaled = scaler.fit_transform(np.array(data).reshape(-1, 1))
data_normalized = normalized_scaler.fit_transform(np.array(data).reshape(-1, 1))

print("Standardized Data:\n", data_scaled.flatten())
print("Normalized Data:\n", data_normalized.flatten())


---

9. Feature Encoding

Encoding categorical data into numerical form.

from sklearn.preprocessing import LabelEncoder, OneHotEncoder

labels = ['Red', 'Blue', 'Green', 'Red', 'Green']
label_encoder = LabelEncoder()
encoded_labels = label_encoder.fit_transform(labels)

print("Label Encoded:", encoded_labels)

# One-Hot Encoding
df = pd.DataFrame({'Color': labels})
df_encoded = pd.get_dummies(df, columns=['Color'])

print(df_encoded)


---

10. Statistical Inference – Parameter Estimation and Hypothesis Testing

Example of Hypothesis Testing (T-test).

sample1 = np.random.normal(100, 15, 50)
sample2 = np.random.normal(110, 15, 50)

t_stat, p_value = stats.ttest_ind(sample1, sample2)

print("T-statistic:", t_stat)
print("P-value:", p_value)


---

These codes cover fundamental statistical concepts applied in Data Science. Let me know if you need further explanations or modifications!





Formula for Correlation Matrix

The correlation matrix is calculated using Pearson’s correlation coefficient formula:

r_{xy} = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum (x_i - \bar{x})^2} \sqrt{\sum (y_i - \bar{y})^2}}

Where:

 is the correlation coefficient between variables  and .

 are individual data points.

 are the means of  and .

The denominator is the product of the standard deviations of  and .


In Python, we can compute the correlation matrix using pandas:

import pandas as pd
import numpy as np

# Sample data
data = {
    'X1': [10, 20, 30, 40, 50],
    'X2': [15, 25, 35, 45, 55],
    'X3': [20, 30, 40, 50, 60]
}

df = pd.DataFrame(data)

# Compute Correlation Matrix
correlation_matrix = df.corr()
print("Correlation Matrix:\n", correlation_matrix)


---

Variance Inflation Factor (VIF) to Detect Multicollinearity

VIF Formula:

VIF_i = \frac{1}{1 - R^2_i}

Where:

 is the coefficient of determination for the regression of variable  on all other predictors.


A VIF > 10 suggests severe multicollinearity, meaning the predictor is highly correlated with other variables.

Python Code to Compute VIF

from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

# Add constant for VIF calculation
X = add_constant(df)

# Compute VIF for each feature
vif_data = pd.DataFrame()
vif_data["Feature"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

print("Variance Inflation Factor (VIF):\n", vif_data)


---

Interpreting VIF Values

VIF < 5 → Low multicollinearity (Good)

VIF between 5 and 10 → Moderate multicollinearity (Consider dropping variables)

VIF > 10 → High multicollinearity (Strong correlation, needs fixing)


If high VIF is detected, you can:

1. Drop one of the correlated variables.


2. Use Principal Component Analysis (PCA) for dimensionality reduction.


3. Use Regularization (Lasso Regression) to penalize correlated features.



Let me know if you need further analysis!




Below is a self-contained Python script that demonstrates how to:

1. Generate some sample data.


2. Compute the correlation and covariance matrices.


3. Create heatmap charts of these matrices using Seaborn.



import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# --------------------------
# 1. Generate Sample Data
# --------------------------
np.random.seed(42)
data = {
    'X1': np.random.randint(10, 100, 10),
    'X2': np.random.randint(20, 110, 10),
    'X3': np.random.randint(30, 120, 10),
    'X4': np.random.randint(40, 130, 10)
}
df = pd.DataFrame(data)

# --------------------------
# 2. Compute Matrices
# --------------------------
# Correlation Matrix
correlation_matrix = df.corr()

# Covariance Matrix
covariance_matrix = df.cov()

# --------------------------
# 3. Create Heatmap Charts
# --------------------------
# Plot Correlation Matrix
plt.figure(figsize=(10, 4))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Matrix")
plt.show()

# Plot Covariance Matrix
plt.figure(figsize=(10, 4))
sns.heatmap(covariance_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Covariance Matrix")
plt.show()

Explanation:

1. Data Creation: We use np.random.seed(42) for reproducible results and generate random integers to simulate four features (X1, X2, X3, and X4).


2. Correlation Matrix:

We calculate pairwise correlations with df.corr().

Correlation values range from -1 (perfect negative correlation) to 1 (perfect positive correlation).



3. Covariance Matrix:

We calculate pairwise covariances with df.cov().

Covariance indicates how two variables move together but is scale-dependent.



4. Heatmaps:

We use seaborn.heatmap() to create visually appealing heatmaps for both matrices.

annot=True displays the numeric values on each cell.

cmap='coolwarm' defines the color scale.

fmt=".2f" ensures values are formatted to two decimal places.




Run this script, and you will see two plots: one for the Correlation Matrix and another for the Covariance Matrix.



Below are six separate code examples demonstrating how to:

1. Compute and display skewness and kurtosis


2. Show value counts for all categorical columns


3. Create a pair plot


4. Perform PCA


5. Visualize PCA results


6. Show feature importance



Feel free to mix and match or modify these code snippets as needed for your specific dataset.


---

1. Code for Showing Skewness and Kurtosis

import pandas as pd
import numpy as np
from scipy.stats import skew, kurtosis

# Sample DataFrame
df = pd.DataFrame({
    'A': np.random.normal(50, 10, 100),
    'B': np.random.gamma(shape=2, scale=1.5, size=100),
    'C': np.random.uniform(0, 100, 100)
})

# Calculate and display skewness & kurtosis for numeric columns
numeric_cols = df.select_dtypes(include=[np.number]).columns

for col in numeric_cols:
    col_skew = skew(df[col])
    col_kurt = kurtosis(df[col])
    print(f"Column: {col}")
    print(f"  Skewness: {col_skew:.4f}")
    print(f"  Kurtosis: {col_kurt:.4f}\n")

Explanation:

We use scipy.stats.skew and scipy.stats.kurtosis to calculate skewness and kurtosis for each numeric column in the DataFrame.



---

2. Code for Value Counts of All Categorical Columns

import pandas as pd

# Sample DataFrame with categorical columns
df = pd.DataFrame({
    'Color': ['Red', 'Blue', 'Red', 'Green', 'Blue', 'Green', 'Red'],
    'Shape': ['Circle', 'Square', 'Circle', 'Triangle', 'Square', 'Circle', 'Square'],
    'Numeric': [10, 20, 30, 40, 50, 60, 70]  # Numeric column
})

# Select only categorical columns (object or category dtype)
categorical_cols = df.select_dtypes(include=['object', 'category']).columns

for col in categorical_cols:
    print(f"Value counts for column '{col}':")
    print(df[col].value_counts())
    print("-----")

Explanation:

We select columns of type object or category.

We then call value_counts() on each of these columns to see how many times each category appears.



---

3. Code for Pair Plot

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Sample DataFrame
df = pd.DataFrame({
    'A': np.random.normal(50, 10, 100),
    'B': np.random.normal(60, 5, 100),
    'C': np.random.normal(70, 15, 100),
    'Category': np.random.choice(['Type1', 'Type2'], 100)  # Categorical column
})

# Create a pair plot
sns.pairplot(df, hue='Category')  # 'hue' uses the categorical column for color
plt.show()

Explanation:

sns.pairplot automatically creates pairwise scatter plots (and histograms on the diagonal) for all numeric columns.

If you have a categorical column (like 'Category'), you can specify it in the hue parameter to color the data points accordingly.



---

4. Code for PCA

import pandas as pd
import numpy as np
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Sample DataFrame
df = pd.DataFrame({
    'A': np.random.normal(50, 10, 100),
    'B': np.random.normal(60, 5, 100),
    'C': np.random.normal(70, 15, 100),
    'D': np.random.normal(80, 20, 100)
})

# 1. Select numeric columns and drop rows with missing values
X = df.select_dtypes(include=[np.number]).dropna()

# 2. Standardize the data (often recommended before PCA)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 3. Perform PCA (let's keep 2 components for visualization)
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# 4. Print explained variance ratio
print("Explained Variance Ratio of each principal component:")
print(pca.explained_variance_ratio_)

Explanation:

StandardScaler is used to standardize features before PCA (especially important when variables are on different scales).

PCA(n_components=2) reduces the data to 2 principal components.



---

5. Code for Showing PCA Results in Charts

import matplotlib.pyplot as plt

# Assuming you have X_pca from the previous code
# Let's create a scatter plot of the first two principal components

plt.figure(figsize=(8,6))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c='blue', alpha=0.7)
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA - First 2 Principal Components')
plt.show()

Explanation:

We scatter-plot the first component (X-axis) against the second component (Y-axis).

Color and alpha settings are optional but help visualize the data.



---

6. Code Showing Feature Importance

Below is an example using a Random Forest classifier. If you have a regression problem, you can use RandomForestRegressor similarly.

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# Sample DataFrame
df = pd.DataFrame({
    'Feature1': np.random.normal(50, 10, 100),
    'Feature2': np.random.normal(60, 5, 100),
    'Feature3': np.random.normal(70, 15, 100),
    'Label': np.random.choice([0, 1], 100)
})

# Split into features (X) and target (y)
X = df.drop('Label', axis=1)
y = df['Label']

# Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and fit the RandomForest model
rf = RandomForestClassifier(random_state=42)
rf.fit(X_train, y_train)

# Feature importance
importances = rf.feature_importances_

# Print feature importances
for feature_name, importance_value in zip(X.columns, importances):
    print(f"{feature_name}: {importance_value:.4f}")

# (Optional) Plot the feature importances
import matplotlib.pyplot as plt

plt.bar(x=X.columns, height=importances, color='green')
plt.xlabel('Features')
plt.ylabel('Importance')
plt.title('Random Forest Feature Importances')
plt.show()

Explanation:

RandomForestClassifier provides an attribute feature_importances_ after fitting, which shows the relative importance of each feature in predicting the target.

We optionally visualize them using a bar chart.



---

Putting It All Together

You can combine or adapt any of these snippets depending on your use case and dataset. If you have any questions or need further customization, just let me know!



Assignment 1: Central Limit Theorem Tutorial in Python

This tutorial will demonstrate the Central Limit Theorem (CLT) using simple, easy-to-understand Python code. The process will be:

1. Generate a skewed dataset.


2. Draw multiple random samples from the dataset.


3. Compute sample means for different sample sizes.


4. Show how the distribution of sample means approaches a normal distribution as sample size increases.



Python Code for CLT Tutorial

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Step 1: Generate a skewed dataset (Exponential Distribution)
np.random.seed(42)
data = np.random.exponential(scale=2, size=1000)

# Plot the original skewed distribution
plt.figure(figsize=(7, 4))
sns.histplot(data, bins=30, kde=True, color='red')
plt.title("Original Skewed Data (Exponential Distribution)")
plt.xlabel("Value")
plt.ylabel("Frequency")
plt.show()

# Function to generate sample means
def sample_means_distribution(sample_size, num_samples=1000):
    sample_means = [np.mean(np.random.choice(data, size=sample_size, replace=True)) for _ in range(num_samples)]
    return sample_means

# Step 2: Show different sample sizes & their distributions
sample_sizes = [5, 30, 100]
fig, axes = plt.subplots(1, 3, figsize=(15, 4))

for i, n in enumerate(sample_sizes):
    sample_means = sample_means_distribution(sample_size=n)
    sns.histplot(sample_means, bins=30, kde=True, ax=axes[i], color=['blue', 'green', 'purple'][i])
    axes[i].set_title(f"Sample Size = {n}")
    axes[i].set_xlabel("Sample Mean")
    axes[i].set_ylabel("Frequency")

plt.suptitle("Central Limit Theorem: Distribution of Sample Means")
plt.tight_layout()
plt.show()

What This Code Does:

1. Creates a skewed dataset using an exponential distribution.


2. Takes multiple random samples of different sizes (5, 30, 100).


3. Computes the means of these samples.


4. Plots histograms to show how sample means follow a normal distribution as sample size increases.




---

Assignment 2: Normal Distribution Tutorial in Python

This tutorial will explain how normal distribution works, how we can generate a normal distribution in Python, and what its properties are.

Python Code for Normal Distribution Tutorial

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import norm

# Step 1: Generate Normally Distributed Data
np.random.seed(42)
mu, sigma = 0, 1  # Mean and Standard Deviation
data = np.random.normal(mu, sigma, 1000)

# Step 2: Plot the Normal Distribution
plt.figure(figsize=(7, 4))
sns.histplot(data, bins=30, kde=True, color='blue')
plt.title("Normal Distribution (Mean = 0, Std Dev = 1)")
plt.xlabel("Value")
plt.ylabel("Frequency")
plt.show()

# Step 3: Show Empirical Rule (68-95-99.7 Rule)
x = np.linspace(mu - 4*sigma, mu + 4*sigma, 100)
y = norm.pdf(x, mu, sigma)

plt.figure(figsize=(7, 4))
plt.plot(x, y, color='blue', linewidth=2, label="Normal Curve")
plt.fill_between(x, y, where=(x >= mu - sigma) & (x <= mu + sigma), color='blue', alpha=0.3, label="68% (1σ)")
plt.fill_between(x, y, where=(x >= mu - 2*sigma) & (x <= mu + 2*sigma), color='green', alpha=0.3, label="95% (2σ)")
plt.fill_between(x, y, where=(x >= mu - 3*sigma) & (x <= mu + 3*sigma), color='red', alpha=0.3, label="99.7% (3σ)")

plt.axvline(mu, color='black', linestyle="--", linewidth=1, label="Mean (μ)")
plt.axvline(mu + sigma, color='gray', linestyle="--", linewidth=1)
plt.axvline(mu - sigma, color='gray', linestyle="--", linewidth=1)
plt.axvline(mu + 2*sigma, color='gray', linestyle="--", linewidth=1)
plt.axvline(mu - 2*sigma, color='gray', linestyle="--", linewidth=1)
plt.axvline(mu + 3*sigma, color='gray', linestyle="--", linewidth=1)
plt.axvline(mu - 3*sigma, color='gray', linestyle="--", linewidth=1)

plt.xlabel("X Values")
plt.ylabel("Probability Density")
plt.title("Empirical Rule (68-95-99.7) for Normal Distribution")
plt.legend()
plt.grid()
plt.show()

What This Code Does:

1. Generates a normal distribution with mean = 0 and standard deviation = 1.


2. Plots the normal distribution histogram with a KDE curve.


3. Illustrates the Empirical Rule (68-95-99.7%) using shaded areas.




---

Summary

CLT Tutorial shows how a skewed distribution's sample means follow a normal distribution as sample size increases.

Normal Distribution Tutorial explains how normal distribution works and highlights the empirical rule.


These codes are beginner-friendly, visual, and interactive. Let me know if you need further customizations!

