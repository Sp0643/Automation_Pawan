from gensim.models import Word2Vec
import nltk
import numpy as np

# Tokenize the cleaned text
df3['clean_text_tok'] = [nltk.word_tokenize(i) for i in df3['cleaned_text']]

# Train Word2Vec model
model = Word2Vec(df3['clean_text_tok'], min_count=1)

# Create word-to-vector dictionary
w2v = dict(zip(model.wv.index_to_key, model.wv.vectors))  # for Gensim >= 4.x

# Define MeanEmbeddingVectorizer class (already done in your code)

# Use the vectorizer
modelw = MeanEmbeddingVectorizer(w2v)
X_train_vectors_w2v = modelw.transform(X_train_tok)
X_val_vectors_w2v = modelw.transform(X_test_tok)



class MeanEmbeddingVectorizer(object):
    def __init__(self, word2vec):
        self.word2vec = word2vec
        self.dim = len(next(iter(word2vec.values())))  # dimensionality of vectors

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        import numpy as np
        return np.array([
            np.mean(
                [self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)],
                axis=0
            )
            for words in X
        ])



To convert your current binary classification pipeline into a multiclass classification model using logistic regression and Word2Vec embeddings, you just need a few changes.


---

Step-by-Step Updates:

1. Label Encode Multiclass Target:

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
y_train_enc = le.fit_transform(y_train)
y_test_enc = le.transform(y_test)


---

2. Train Multiclass Logistic Regression:

Use multi_class='multinomial' and solver='lbfgs' (commonly used for multiclass):

from sklearn.linear_model import LogisticRegression

lr_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)
lr_model.fit(X_train_vectors_w2v, y_train_enc)


---

3. Predictions and Evaluation:

from sklearn.metrics import classification_report, confusion_matrix

y_pred = lr_model.predict(X_val_vectors_w2v)

print("Classification Report:")
print(classification_report(y_test_enc, y_pred, target_names=le.classes_))

print("Confusion Matrix:")
print(confusion_matrix(y_test_enc, y_pred))


---

Optional: Predict Probabilities & AUC (one-vs-rest)

from sklearn.metrics import roc_auc_score
y_proba = lr_model.predict_proba(X_val_vectors_w2v)

# For multiclass AUC (macro-average)
auc_score = roc_auc_score(y_test_enc, y_proba, multi_class='ovr')
print("Multiclass ROC AUC Score:", auc_score)


---

Let me know:

How many unique classes are in your target?

If you're using TF-IDF instead of Word2Vec for vectorization.


I can adapt the code if needed.




To convert your code from TF-IDF to Word2Vec, you can use the gensim library. Hereâ€™s how to update your code for Word2Vec embedding:

Updated Code for Word2Vec

from gensim.models import Word2Vec
from nltk.tokenize import word_tokenize
import nltk

# Ensure required NLTK data is available
nltk.download('punkt')

# Tokenize the clean text
df1['tokens'] = df1['clean_text'].apply(word_tokenize)

# Train Word2Vec model
w2v_model = Word2Vec(sentences=df1['tokens'], vector_size=100, window=5, min_count=1, workers=4)

# Function to get average Word2Vec vector for each document
def get_avg_vector(tokens, model, vector_size):
    vec = [model.wv[word] for word in tokens if word in model.wv]
    return np.mean(vec, axis=0) if vec else np.zeros(vector_size)

import numpy as np

# Apply the function to get document vectors
df1['w2v_vector'] = df1['tokens'].apply(lambda x: get_avg_vector(x, w2v_model, 100))

Notes:

Replace 100 in vector_size=100 and get_avg_vector(..., 100) if you want a different dimensionality.

The resulting column w2v_vector contains NumPy arrays of shape (100,).


Let me know if you want to use pre-trained Word2Vec models like Google News instead.

