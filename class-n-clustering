from gensim.models import Word2Vec
import nltk
import numpy as np

# Tokenize the cleaned text
df3['clean_text_tok'] = [nltk.word_tokenize(i) for i in df3['cleaned_text']]

# Train Word2Vec model
model = Word2Vec(df3['clean_text_tok'], min_count=1)

# Create word-to-vector dictionary
w2v = dict(zip(model.wv.index_to_key, model.wv.vectors))  # for Gensim >= 4.x

# Define MeanEmbeddingVectorizer class (already done in your code)

# Use the vectorizer
modelw = MeanEmbeddingVectorizer(w2v)
X_train_vectors_w2v = modelw.transform(X_train_tok)
X_val_vectors_w2v = modelw.transform(X_test_tok)



class MeanEmbeddingVectorizer(object):
    def __init__(self, word2vec):
        self.word2vec = word2vec
        self.dim = len(next(iter(word2vec.values())))  # dimensionality of vectors

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        import numpy as np
        return np.array([
            np.mean(
                [self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)],
                axis=0
            )
            for words in X
        ])