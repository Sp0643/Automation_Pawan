from gensim.models import Word2Vec
import nltk
import numpy as np

# Tokenize the cleaned text
df3['clean_text_tok'] = [nltk.word_tokenize(i) for i in df3['cleaned_text']]

# Train Word2Vec model
model = Word2Vec(df3['clean_text_tok'], min_count=1)

# Create word-to-vector dictionary
w2v = dict(zip(model.wv.index_to_key, model.wv.vectors))  # for Gensim >= 4.x

# Define MeanEmbeddingVectorizer class (already done in your code)

# Use the vectorizer
modelw = MeanEmbeddingVectorizer(w2v)
X_train_vectors_w2v = modelw.transform(X_train_tok)
X_val_vectors_w2v = modelw.transform(X_test_tok)



class MeanEmbeddingVectorizer(object):
    def __init__(self, word2vec):
        self.word2vec = word2vec
        self.dim = len(next(iter(word2vec.values())))  # dimensionality of vectors

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        import numpy as np
        return np.array([
            np.mean(
                [self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)],
                axis=0
            )
            for words in X
        ])



To convert your current binary classification pipeline into a multiclass classification model using logistic regression and Word2Vec embeddings, you just need a few changes.


---

Step-by-Step Updates:

1. Label Encode Multiclass Target:

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
y_train_enc = le.fit_transform(y_train)
y_test_enc = le.transform(y_test)


---

2. Train Multiclass Logistic Regression:

Use multi_class='multinomial' and solver='lbfgs' (commonly used for multiclass):

from sklearn.linear_model import LogisticRegression

lr_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)
lr_model.fit(X_train_vectors_w2v, y_train_enc)


---

3. Predictions and Evaluation:

from sklearn.metrics import classification_report, confusion_matrix

y_pred = lr_model.predict(X_val_vectors_w2v)

print("Classification Report:")
print(classification_report(y_test_enc, y_pred, target_names=le.classes_))

print("Confusion Matrix:")
print(confusion_matrix(y_test_enc, y_pred))


---

Optional: Predict Probabilities & AUC (one-vs-rest)

from sklearn.metrics import roc_auc_score
y_proba = lr_model.predict_proba(X_val_vectors_w2v)

# For multiclass AUC (macro-average)
auc_score = roc_auc_score(y_test_enc, y_proba, multi_class='ovr')
print("Multiclass ROC AUC Score:", auc_score)


---

Let me know:

How many unique classes are in your target?

If you're using TF-IDF instead of Word2Vec for vectorization.


I can adapt the code if needed.




To convert your code from TF-IDF to Word2Vec, you can use the gensim library. Hereâ€™s how to update your code for Word2Vec embedding:

Updated Code for Word2Vec

from gensim.models import Word2Vec
from nltk.tokenize import word_tokenize
import nltk

# Ensure required NLTK data is available
nltk.download('punkt')

# Tokenize the clean text
df1['tokens'] = df1['clean_text'].apply(word_tokenize)

# Train Word2Vec model
w2v_model = Word2Vec(sentences=df1['tokens'], vector_size=100, window=5, min_count=1, workers=4)

# Function to get average Word2Vec vector for each document
def get_avg_vector(tokens, model, vector_size):
    vec = [model.wv[word] for word in tokens if word in model.wv]
    return np.mean(vec, axis=0) if vec else np.zeros(vector_size)

import numpy as np

# Apply the function to get document vectors
df1['w2v_vector'] = df1['tokens'].apply(lambda x: get_avg_vector(x, w2v_model, 100))

Notes:

Replace 100 in vector_size=100 and get_avg_vector(..., 100) if you want a different dimensionality.

The resulting column w2v_vector contains NumPy arrays of shape (100,).


Let me know if you want to use pre-trained Word2Vec models like Google News instead.




To apply KMeans clustering to the output of Word2Vec vectors, you just need to replace the X_tfidf in your current KMeans code with the Word2Vec matrix you've created from the earlier code. Here's how you can update and apply it:

Step-by-Step Code:

from sklearn.cluster import KMeans
import numpy as np

# Step 1: Convert Word2Vec vectors from series to a matrix (2D array)
X_w2v = np.stack(df1['w2v_vector'].values)

# Step 2: Define and fit KMeans
num_clusters = 5
kmeans = KMeans(n_clusters=num_clusters, random_state=42)
kmeans.fit(X_w2v)

# Step 3: Assign cluster labels to the dataframe
df1['cluster_kmeans'] = kmeans.labels_

Notes:

np.stack() is used to convert the list/Series of vectors into a proper 2D array.

This code assumes you already ran the Word2Vec logic and have a column w2v_vector in df1.


Let me know if you want to visualize the clusters or find the optimal number of clusters (Elbow method etc.).



Sure! Here's how you can create a separate text corpus for each KMeans cluster using the clean_text column and the assigned cluster_kmeans labels:

Code to Create Corpus per Cluster

# Group text data by KMeans clusters and join into a single string per cluster
cluster_corpora = (
    df1.groupby('cluster_kmeans')['clean_text']
    .apply(lambda texts: ' '.join(texts.dropna()))
    .to_dict()
)

# Display the corpora
for cluster_id, corpus in cluster_corpora.items():
    print(f"\n--- Cluster {cluster_id} Corpus ---\n")
    print(corpus[:500], "...")  # Print first 500 characters for brevity

Explanation:

groupby('cluster_kmeans'): Groups rows by cluster label.

.apply(lambda texts: ' '.join(...)): Joins all clean_text in each cluster.

to_dict(): Converts the grouped result into a dictionary with keys as cluster IDs and values as corpora.


Let me know if you'd like to save these corpora to files or analyze them for keywords, topics, etc.





from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd

# Set your n-gram range (e.g., bigrams (2,2), trigrams (3,3), etc.)
ngram_range = (2, 2)  # For bigrams
top_n = 10            # Top 10 most frequent n-grams

# Function to extract top n-grams from text
def get_top_ngrams(text, ngram_range, top_n):
    vec = CountVectorizer(ngram_range=ngram_range, stop_words='english')
    bag_of_words = vec.fit_transform([text])
    sum_words = bag_of_words.sum(axis=0)
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)
    return words_freq[:top_n]

# Create a DataFrame to hold top n-grams for each cluster
ngram_summary = {}

for cluster_id, corpus in cluster_corpora.items():
    ngram_summary[f'Cluster {cluster_id}'] = get_top_ngrams(corpus, ngram_range, top_n)

# Convert to DataFrame for comparison
ngram_df = pd.DataFrame(ngram_summary)

# Display the n-gram comparison
import ace_tools as tools; tools.display_dataframe_to_user(name="Top N-grams by Cluster", dataframe=ngram_df)