Teaching PySpark to your team can be highly effective when paired with clear explanations and practical examples. Hereâ€™s a guide to 20 core PySpark concepts, including their explanations and code snippets:


---

1. Setting up PySpark

Explanation: Learn to initialize a Spark session.


from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("PySpark Basics") \
    .getOrCreate()


---

2. Creating an RDD

Explanation: Understand how to create and manipulate Resilient Distributed Datasets (RDDs).


data = [1, 2, 3, 4, 5]
rdd = spark.sparkContext.parallelize(data)
print(rdd.collect())


---

3. DataFrames: The Core API

Explanation: DataFrames are distributed tables with optimized query execution.


data = [("Alice", 34), ("Bob", 36), ("Cathy", 30)]
columns = ["Name", "Age"]
df = spark.createDataFrame(data, columns)
df.show()


---

4. Schema and Data Types

Explanation: Define schemas explicitly for better control over data.


from pyspark.sql.types import StructType, StructField, StringType, IntegerType

schema = StructType([
    StructField("Name", StringType(), True),
    StructField("Age", IntegerType(), True)
])

df = spark.createDataFrame(data, schema=schema)
df.printSchema()


---

5. Reading Data

Explanation: Load data from various file formats (CSV, JSON, Parquet).


df = spark.read.csv("data.csv", header=True, inferSchema=True)
df.show()


---

6. Writing Data

Explanation: Save DataFrame into different formats.


df.write.csv("output_path", header=True)


---

7. DataFrame Operations

Explanation: Perform basic operations like select, filter, and groupBy.


df.select("Name").show()
df.filter(df["Age"] > 30).show()
df.groupBy("Age").count().show()


---

8. SQL Queries

Explanation: Use SQL for querying DataFrames.


df.createOrReplaceTempView("people")
result = spark.sql("SELECT * FROM people WHERE Age > 30")
result.show()


---

9. Joins

Explanation: Combine DataFrames using inner, left, and right joins.


data2 = [("Alice", "HR"), ("Bob", "Finance")]
df2 = spark.createDataFrame(data2, ["Name", "Department"])
df.join(df2, "Name").show()


---

10. Window Functions

Explanation: Perform operations over a sliding window of rows.


from pyspark.sql.window import Window
from pyspark.sql.functions import row_number

windowSpec = Window.partitionBy("Department").orderBy("Age")
df.withColumn("row_number", row_number().over(windowSpec)).show()


---

11. Aggregations

Explanation: Perform aggregations like sum, avg, count.


from pyspark.sql.functions import avg
df.groupBy("Department").agg(avg("Age").alias("Average Age")).show()


---

12. Handling Nulls

Explanation: Manage missing values effectively.


df.na.fill({"Age": 0}).show()  # Fill null values
df.dropna().show()  # Drop rows with nulls


---

13. UDFs (User Defined Functions)

Explanation: Create custom transformation logic.


from pyspark.sql.functions import udf
from pyspark.sql.types import StringType

def age_group(age):
    return "Adult" if age >= 18 else "Child"

age_group_udf = udf(age_group, StringType())
df.withColumn("Age Group", age_group_udf(df["Age"])).show()


---

14. Broadcast Joins

Explanation: Optimize small DataFrame joins.


from pyspark.sql.functions import broadcast
df.join(broadcast(df2), "Name").show()


---

15. Caching and Persisting

Explanation: Cache frequently used DataFrames for performance.


df.cache()
df.count()  # Triggers caching


---

16. Partitioning

Explanation: Control the number of partitions for optimized performance.


df.repartition(4).write.csv("output_path")


---

17. Streaming Data

Explanation: Process real-time data streams.


stream_df = spark.readStream.format("csv").option("path", "input_dir").load()
stream_df.writeStream.format("console").start().awaitTermination()


---

18. Broadcast Variables

Explanation: Share read-only data across nodes.


broadcast_var = spark.sparkContext.broadcast([1, 2, 3])
print(broadcast_var.value)


---

19. Accumulators

Explanation: Aggregate values across worker nodes.


accum = spark.sparkContext.accumulator(0)
rdd.foreach(lambda x: accum.add(x))
print(accum.value)


---

20. Logging and Debugging

Explanation: Enable logging for better debugging.


spark.sparkContext.setLogLevel("DEBUG")


---

Teaching Tips:

1. Use Real-World Examples: Apply concepts to practical problems like sales analysis, web logs, or machine learning pipelines.


2. Encourage Hands-On Practice: Let the team write and run code themselves.


3. Explain Execution Plans: Use .explain() to show the DAG execution for operations.


4. Discuss Optimization: Cover partitioning, caching, and efficient joins.



Let me know if you need tailored examples or more advanced topics!

